apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx

---
#kubectl get pod nginx -o yaml

# kubectl describe



# You cannot change the namespace, name, uid, or creationTimestamp fields; the generation
# You can add spec.containers[*].image, spec.initContainers[*].image, spec.activeDeadlineSeconds or spec.toleration
---


# Go inside a container
# kubectl exec -it nginx -- bash

# Storage in pods
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo-for-storage
spec:
  containers:
    - name: debug
      image: rkalluru/learnk8s:rhel9-bare
      volumeMounts:
        - mountPath: /mnt/html
          name: html-volume
    - name: nginx
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: html-volume
  volumes:
    - name: html-volume
      emptyDir: {}


# kubectl exec -it -c debug nginx -- bash
# kubectl exec -it -c nginx nginx -- bash

# network in pods
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo-for-network
spec:
  containers:
    - name: debug
      image: rkalluru/learnk8s:rhel9-bare
    - name: nginx
      image: nginx

# Init containers
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo-for-init-container
spec:
  initContainers:
    - name: html-generator
      image: rkalluru/learnk8s:init-container-html-generator
      volumeMounts:
        - mountPath: /mnt/html
          name: html-volume
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: html-volume
  volumes:
    - name: html-volume
      emptyDir: {}

# sidecar containers
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo-for-side-car-container
spec:
  containers:
    - name: html-generator
      image: rkalluru/learnk8s:side-car-html-generator
      volumeMounts:
        - mountPath: /mnt/html
          name: html-volume
    - name: nginx
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: html-volume
  volumes:
    - name: html-volume
      emptyDir: {}

# Labels
apiVersion: v1
kind: Pod
metadata:
  name: p1
  labels:
    project: example
    application: c1
spec:
  containers:
    - name: c1
      image: rkalluru/learnk8s:rhel9-bare
---
apiVersion: v1
kind: Pod
metadata:
  name: p2
  labels:
    project: example
    application: c2
spec:
  containers:
    - name: c2
      image: rkalluru/learnk8s:rhel9-bare

# kubectl get pods --show-labels
#kubectl get pods -l project=example
#kubectl get pods -l application=c2



# Annotations




# Replica set
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-demo
  labels:
    project: example
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx


# kubectl get rs
# kubectl describe rs/rs-demo
# kubectl get pods frontend-gbgfx -o yaml

#   ownerReferences:            This is what we see
#   - apiVersion: apps/v1
#     blockOwnerDeletion: true
#     controller: true
#     kind: ReplicaSet
#     name: frontend
#     uid: e129deca-f864-481b-bb16-b27abfd92292

# Non-Template Pod acquisitions

apiVersion: v1
kind: Pod
metadata:
  name: p3
  labels:
    project: example
    application: nginx
spec:
  containers:
    - name: nginx
      image: nginx

## Isolating Pods from a ReplicaSet
# kubectl patch pod <pod-name> -p '{"metadata":{"ownerReferences":[]}}'
# kubectl patch pod rs-demo-x76jf -p '{"metadata": {"labels": {"app": "demo"}}}'


# scaling a replica set
# kubectl scale replicaset <replicaset-name> --replicas=10
# kubectl scale replicaset <replicaset-name> --replicas=1

# Pod deletion cost  -> To determine which pods to remove first
# kubectl patch pod <pod-name-1> -p '{"metadata":{"annotations":{"controller.kubernetes.io/pod-deletion-cost":"100"}}}'
# -2147483648, 2147483647], Higher the number - Higher the priority

# Example Use Case
#  The different pods of an application could have different utilization levels. On scale down, the application may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application should update controller.kubernetes.io/pod-deletion-cost once before issuing a scale down (setting the annotation to a value proportional to pod utilization level). This works if the application itself controls the down scaling; for example, the driver pod of a Spark deployment.


# kubectl patch pod <pod-name> -p '{"metadata": {"annotations": {"controller.kubernetes.io/pod-deletion-cost": "10"}}}'


# Explain Replication Controller

# What are the limitations of replicaset

# Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rs-demo
  labels:
    project: example
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx


#kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
# kubectl rollout status deployment/nginx-deployment

# kubectl get rs
# kubectl describe deployments

# Deployment Controller

# Dont do Label selector updates

# kubectl set image deployment/nginx-deployment nginx=nginx:1.161
# kubectl rollout status deployment/nginx-deployment

# kubectl rollout history deployment/nginx-deployment

# Rolling back to previous version

# kubectl rollout undo deployment/nginx-deployment


# Scale deployment
# kubectl scale deployment/nginx-deployment --replicas=10

# Rolling Update strategy

# Pods re-create with new config few after few and replaces everything, The default behviour of those values comes from
#
# maxSurge=3, and maxUnavailable=2
#
# Lets see these values from the existing deployment.


apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3            # Maximum number of pods that can be created above the desired number of pods during an update
      maxUnavailable: 2       # Maximum number of pods that can be unavailable during the update
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app-image

# kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
# kubectl rollout pause deployment/nginx-deployment
# kubectl rollout history deployment/nginx-deployment



# Pausing and Resuming a rollout of a Deployment
You can pause , Make some updates and then resume the rollout

# kubectl set image deployment/nginx-deployment nginx=nginx:1.17
kubectl rollout resume deployment/nginx-deployment

kubectl get rs --watch


### Deployment status

## Deployment History - Clean up Policy

.spec.revisionHistoryLimit to 0, a

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
spec:
  replicas: 3
  revisionHistoryLimit: 0  # This will retain no old revisions of the Deployment
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app-image

## Strategy
.spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be "Recreate" or "RollingUpdate". "RollingUpdate" is the default value.

Recreate - Delete pod and create
Rolling Update - Create pod and delete

#########################################################################################################################
It is inevitable and understand that to run a application then we need to go with Deployments.
1. To maintain replicas
2. To rotate pods when there is an image change

So lets make the project Running with the deployments.deployment







apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-deployment
spec:
  replicas: 6  # Multiple pods to be created
  selector:
    matchLabels:
      app: example
  template:
    metadata:
      labels:
        app: example
    spec:
      topologySpreadConstraints:
        - maxSkew: 1
          minDomains: 2
          topologyKey: "failure-domain.kubernetes.io/zone"
          whenUnsatisfiable: "DoNotSchedule"
      containers:
        - name: example-container
          image: nginx


Replicas: With 6 replicas in this Deployment, Kubernetes will try to spread these replicas across different zones to meet the constraints.
topologySpreadConstraints:
maxSkew: Ensures that the difference in the number of pods between any two zones is no greater than 1.
minDomains: Specifies that at least 2 zones should be used to distribute the pods.
topologyKey: The pods are spread across zones as indicated by the key failure-domain.kubernetes.io/zone.



labelSelector:
  matchLabels:
    app: web

labelSelector: Only nodes with the label app: web will be considered for spreading the pods.










